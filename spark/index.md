---
layout: page
title: Apache Spark
tags: [about, Jekyll, theme, responsive]
modified: 2014-08-08T20:53:07.573882-04:00
comments: true
image:
  feature: sample-image-7.jpg
---

{% stylesheet mermaid %}

Data is the new currency and it's going to be an important factor for any business looking to thrive in an increasingly competitive landscape. There are quite a few homegrown data solutions that end up creating a large cost to the business due to natural complexites of analytics problems. That is why you should take a serious look at a unified data framework.

## Why you need a unified data framework

Most businesses that have built a technology platform will eventually include analytics component in their solution. These solutions typically involve the implementation of one or more of the following:

* Real-time dashboards
* Personalization for your users or customers
* Data warehousing and business intelligence
* Complex data integration.

However, these software problems are non-trivial to solve and usually involve a significant amount of effort and cost to develop. A relatively new technology, known as [Apache Spark](http://spark.apache.org/), can help ease the pain of building out these systems.

## Simplify your stack by using a single technology

The spark framework exposes a set of high-level libraries that enable you to _quickly_ build out analytics features. Such as the ability to interface with the data through SQL as well as machine learning libraries that can all be executed as the data streams in. 

> Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can combine these libraries seamlessly in the same application.

<img style="float: center; PADDING-LEFT: 25px; PADDING-TOP: 25px; PADDING-BOTTOM: 25px; height: 175px" src="/assets/images/spark-stack.png">

## Reduce cost by saving time and effort

All of these libraries are built on top of a very fast compute engine that allow you to scale out the system by simply adding worker nodes into the pool. 

> Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.

That means that your engineering team spends less time setting up and maintaining infrastructure and more time building applications that deliver value for your business.

> Spark offers over 80 high-level operators that make it easy to build parallel apps. And you can use it interactively from the Scala, Python and R shells.

## Fault-tolerance and reliability is built-in

The complexities of distributed fault-tolerant processing are abstracted out by providing [simple APIs](http://spark.apache.org/docs/latest/programming-guide.html) to the application developer. The primary abstraction is known as [Resilient Distributed Datasets (RDDs)](https://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds), and more recently Spark introduced [DataFrames](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes) as a way for developers to work with tabular data.

## Time to make your data ubiquitous 

If you're interested in working with me to develop your next generation analytics platform, please email me at [mike.trienis@quickinsights.io](mailto:mike.trienis@quickinsights.io)

